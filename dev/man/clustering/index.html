<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Clustering Strategies · FastBEAST.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">FastBEAST.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../gstarted/">Getting Started</a></li><li><span class="tocitem">Manual</span><ul><li class="is-active"><a class="tocitem" href>Clustering Strategies</a><ul class="internal"><li><a class="tocitem" href="#Box-Tree"><span>Box Tree</span></a></li><li><a class="tocitem" href="#K-Means-Clustering-Tree"><span>K-Means Clustering Tree</span></a></li></ul></li><li><a class="tocitem" href="../hmatrix/">H-Matrix</a></li><li><a class="tocitem" href="../aca/">Adaptive Cross Aproximation</a></li></ul></li><li><a class="tocitem" href="../../functions/">Types and Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Clustering Strategies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Clustering Strategies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sbadrian/FastBEAST/blob/main/docs/src/man/clustering.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Clustering-Strategies"><a class="docs-heading-anchor" href="#Clustering-Strategies">Clustering Strategies</a><a id="Clustering-Strategies-1"></a><a class="docs-heading-anchor-permalink" href="#Clustering-Strategies" title="Permalink"></a></h1><p>For the algorithms defined in this package the given set of datapoints have to be sorted in an algebraic tree.  Therefor the following strategies can be used. </p><h2 id="Box-Tree"><a class="docs-heading-anchor" href="#Box-Tree">Box Tree</a><a id="Box-Tree-1"></a><a class="docs-heading-anchor-permalink" href="#Box-Tree" title="Permalink"></a></h2><h2 id="K-Means-Clustering-Tree"><a class="docs-heading-anchor" href="#K-Means-Clustering-Tree">K-Means Clustering Tree</a><a id="K-Means-Clustering-Tree-1"></a><a class="docs-heading-anchor-permalink" href="#K-Means-Clustering-Tree" title="Permalink"></a></h2><h4 id="Definition"><a class="docs-heading-anchor" href="#Definition">Definition</a><a id="Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Definition" title="Permalink"></a></h4><p>The K-Means algorithm is a clustering strategy for n-dimensional spaces, in which each cluster is represented by its center. In this case the algorithm is implemented for a two or three dimensional euclidean space.</p><h4 id="Idea"><a class="docs-heading-anchor" href="#Idea">Idea</a><a id="Idea-1"></a><a class="docs-heading-anchor-permalink" href="#Idea" title="Permalink"></a></h4><p>The goal of the algorithm is to divide the dataset in k clusters, which are then alternately divided in k clusters.  The procedure is as follows: At first there are k points chosen from the dataset as centers. Then each point is sorted to its closest center. For each resulting cluster the center is recalculated by its points and all points are resorted to the new centers. This step is repeated for a given number of iterations. The whole process is alternately repeated for each cluster.  </p><h4 id="Algorithm"><a class="docs-heading-anchor" href="#Algorithm">Algorithm</a><a id="Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithm" title="Permalink"></a></h4><p>Given is a random distribution of datapoints in 2D, which should be sorted in a binary tree.  Two points out of the dataset are chosen as the first centers. For each of the points the euclidean distance to both centers is calculated by:</p><p class="math-container">\[    dist = norm(x - c_i)\quad for\enspace i = 1,2,..,k\]</p><p>With <span>$x$</span> the location of the datapoint and <span>$c_i$</span> the centers. The point is then added to the closer center.</p><p><img src="../assets/random_distribution.png" alt/> <img src="../assets/sorted_first.png" alt/></p><p>The centers are shown in the first picture in orange and each cluster is represented in the second picture by one color. To achieve a more even distribution between both clusters, the centers are recalculated. Therefor the distance between the center and each point of the cluster has to be minimized. This can be done by taking the mean over all points.</p><p class="math-container">\[    c_i = N_k^{-1} * \sum_{n=1}^{N_k} x_n\]</p><p>The dataset is resorted to the updated centers, resulting in the following distribution.</p><p><img src="../assets/even_distributed.png" alt/></p><p>As it can be seen the updated clusters can result in a more even distribution. These steps can then be repeated for a given amount of iterations.  The final clusters are afterwards taken as new distributions of datapoints and the algorithm can alternately be repeated until clusters with a minimum amount of points or a given number of level is reached. </p><h4 id="Comments"><a class="docs-heading-anchor" href="#Comments">Comments</a><a id="Comments-1"></a><a class="docs-heading-anchor-permalink" href="#Comments" title="Permalink"></a></h4><p>As it can be seen in the example more iterations will in general lead to more equal clusters. By default the wrapped <a href="https://pydatablog.github.io/ParallelKMeans.jl/stable/">ParallelKMeans.jl</a> algorithm is used, which stops iterating when the new center of a cluster is close enough to the last iteration. This prevents unnecessary iterations and thus reduces the required computing time. For non homogenous distributions equal sized clusters can not always be reached, but more iterations will generate in general a better tree structure for the algorithms in this package. For the the algorithms in this package 100 iterations and two children for each level are recommended.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../gstarted/">« Getting Started</a><a class="docs-footer-nextpage" href="../hmatrix/">H-Matrix »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 2 February 2023 13:00">Thursday 2 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
